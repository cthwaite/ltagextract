%        File: task-oriented_parser_evaluation.tex
%     Created: 周五 三月 02 04:00 下午 2012 W EST
% Last Change: 周五 三月 02 04:00 下午 2012 W EST
%
\documentclass[a4paper]{article}
%\usepackage{natbib}
%\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{longtable}
%\usepackage[style=authoryear,natbib=true]{biblatex}
%\bibliography{D:/Research/bib/Parsing}
\usepackage{graphicx}
%\usepackage{gb4e}
\usepackage{lingmacros}
\usepackage{tikz}
\usepackage{tikz-qtree}
\title{Lexicalized Tree Adjoining Grammar Extraction For KBGEN}
\author{Wei Qiu \and Jiri Marsik}
%\date{}
\begin{document}
\maketitle
\begin{abstract}
\end{abstract}
\section{Introduction}


\section{Semantic Annotation}
\label{sec:sem-annot}



\subsection{Annotation guidelines}

\subsection{Problems}


\section{Preprocessing}

There was a long way before we could start work on the problem itself
and we had to write a few auxiliary programs along the way. This
section describes the major steps and the tools we used there.

\subsection{Parsing and headedness}

Our plan is to extract the elementary trees for a TAG from a treebank.
Since our corpus lacks phrase structure information, we ran an
off-the-shelf parser on it. The parser that we used for this task is
the Stanford parser.

At the outset, we had plans to use both the phrase structure and
dependency parses that the Stanford parser could produce. Previous
research has shown that having a dependency parse facilitates the task
of determining headedness, which is usually a subtask of elementary
tree extraction.

Finally, we have found a simpler way of getting the head information
directly in the phrase structure parse trees through a command line
switch in the Stanford parser.


Furthermore, the Stanford parser offers two pre-trained models for
parsing English, an unlexicalized one and a lexicalized one. We have
done the parsing using both of them and try to review their
performance. Their outputs were significantly different and both
performed their own idiosyncratic errors. We judged which of these
errors would likely be more harmful to the following grammar
extraction process and decided to stick with the output of the
unlexicalized parser (though the result of the unlexicalized parsing
almost as just as bad as that of the lexicalized one).

\subsection{Aggregating coordinations}

When we started working on the manual alignment with our self-enforced
constraint to produce only contiguous groups (see section
\ref{sec:sem-annot}), one of the difficulties we faced was
coordination. Consider the following example:

\begin{verbatim}
Electrogenic pumps, which consist of a hydrophobic amino acid,
a polar amino acid and a monomer, create  membrane potential.

(KBGEN-INPUT 
    :TRIPLES (
            (|Electrogenic-Pump21300| |has-part| |Monomer21323|)
            (|Electrogenic-Pump21300| |has-part| |Polar-Amino-Acid21291|)
            (|Electrogenic-Pump21300| |has-part| |Hydrophobic-Amino-Acid21290|)
            (|Create21297| |agent| |Electrogenic-Pump21300|)
            (|Create21297| |result| |Membrane-Potential21299|))
    :INSTANCE-TYPES (
            (|Monomer21323| |instance-of| |Monomer|)
            (|Polar-Amino-Acid21291| |instance-of| |Polar-Amino-Acid|)
            (|Hydrophobic-Amino-Acid21290| |instance-of| |Hydrophobic-Amino-Acid|)
            (|Electrogenic-Pump21300| |instance-of| |Electrogenic-Pump|)
            (|Create21297| |instance-of| |Create|)
            (|Membrane-Potential21299| |instance-of| |Membrane-Potential|)))
\end{verbatim}

To what group should the comma and the conjunction \emph{and} belong,
i.e. what semantics correspond to them and only to them? The
\textbf{has-part} relations belong to the verb \emph{consists}, the
instance declarations belong to the NPs. Yet we need to assign the
coordination constructions to some semantics, otherwise we will not be
able to generate them. To resolve this issue, we have developed a tool
to reify the coordination structures within the triples themselves.

In our first approach, our program would look for every instance where
one entity was in the same relation with multiple entities, such as
above, and converted the triples to the following:

\begin{verbatim}
:TRIPLES (
        (|Coordination2085| |coordinates| |Monomer21323|)
        (|Coordination2085| |coordinates| |Polar-Amino-Acid21291|)
        (|Coordination2086| |coordinates| |Coordination2085|)
        (|Coordination2086| |coordinates| |Hydrophobic-Amino-Acid21290|)
        (|Electrogenic-Pump21300| |has-part| |Coordination2086|)
        (|Create21297| |agent| |Electrogenic-Pump21300|)
        (|Create21297| |result| |Membrane-Potential21299|))
\end{verbatim}

What we did is that we built a very specific kind of binary tree which
covers the coordinated entities (the generated tree is specific in
that it always leans to the right, so it looks more like a linked
list). Here, one \textbf{Coordination} entity,
\textbf{Coordination2085}, subsumes the \textbf{Monomer21323} and the
\textbf{Polar-Amino-Acid21291}. This coordination corresponds to the
conjunction \emph{and}. The second coordination connects another
entity, the \textbf{Hydrophobic-Amino-Acid21290}, to the previously
formed \textbf{Coordination2085}. The resulting
\textbf{Coordination2086} can then serve as the singular object to the
verb \emph{consists}. Now, all of the NP coordinating constructs
(commas and \emph{and}) are easily accounted for. They form singleton
groups that are aligned to a pair of \textbf{coordinates} relations.

Once we made the switch from contiguous groups, we revised our
approach to coordination. Instead of making this hierarchy of
coordination constructs, we just create a single \textbf{Coordination}
entity which encapsulates all of the coordinated entities directly.
Obviously, this would not have been possible for coordinations of more
than 2 constituents using only contiguous groups, since we would not
to place two non-adjacent conjunction markers into one group.

The upsides of this new approach are that it reduces the complexity of
the newly added triples and that it does not overgenerate like the
last approach, which would end up extracting two elementary trees for
conjunction (one for comma and one for \emph{and}), but would not be
able to distinguish between them and use them properly. The only
downside of the new approach is that it needs to learn different
elementary trees for each different arity of conjunction present in
the corpus. However, since the number of coordinated constituents is
always between 2 and 4 in our corpus, this is a non-issue.

Here is the above example, now rendered using the new coordination
aggregation rules:

\begin{verbatim}
    :TRIPLES (
            (|Electrogenic-Pump21300| |has-part| |Coordination2015|)
            (|Coordination2015| |coordinates| |Monomer21323|)
            (|Coordination2015| |coordinates| |Polar-Amino-Acid21291|)
            (|Coordination2015| |coordinates| |Hydrophobic-Amino-Acid21290|)
            (|Create21297| |agent| |Electrogenic-Pump21300|)
            (|Create21297| |result| |Membrane-Potential21299|))
\end{verbatim}

The code for performing the former aggregation method is still present
in the aggregation program, but is no longer used (see function
\texttt{coordinate-objects-list}). Switching to the new method was
just a matter of writing a new function,
\texttt{coordinate-objects-flat}, and calling that one instead of the
former.

\subsection{Normalizing}

Once we had the manually aligned data with the aggregated
coordinations and parsed sentences, we thought ourselves ready to
extract the elementary trees. However, the trees produced by the
Stanford producer do not look very much like derived trees produced by
a TAG.

Xia and Palmer were facing a similar problem (\emph{From Treebanks to
Tree Adjoining Grammars}). We considered applying their approach.
However, their approach is not as simple as it seems on paper.
Consider the following parse tree we got from 

\begin{tikzpicture}[scale=0.5]
\Tree
[.ROOT
  [.S=H
    [.NP [.DT The ] [.NN plasma ] [.NN=H membrane ] ]
    [.VP=H [.VBZ=H moves ]
      [.ADVP [.RB=H away ]
        [.PP [.IN=H from ]
          [.NP [.DT the ] [.NN cell ] [.NN=H wall ] ] ] ]
      [.PP [.IN=H during ]
        [.NP
          [.NP=H [.NNS=H plasmolysis ] ]
          [.PP [.IN=H in ]
            [.NP
              [.NP=H [.DT a ] [.JJ walled ] [.NN=H cell ] ]
              [.PP [.IN=H inside ]
                [.NP [.JJ hypertonic ] [.NN=H solution ] ] ] ] ] ] ]
      [., , ]
      [.S
        [.VP=H [.VBG=H resulting ]
          [.PP [.IN=H in ]
            [.NP [.DT a ] [.VBN damaged ] [.NN=H cell ] ] ] ] ] ]
    [.. . ] ] ]
\end{tikzpicture}

The PP \emph{``during...''} and the S \emph{``resulting...''} with its
preceding comma both modify their parent, the VP of our sentence. We
can imagine that in our implementation of the algorithm, these would
be identified as modifiers and they would be put on different levels
from the argument ADVP, like so.

\begin{tikzpicture}[scale=0.5]
\Tree
[.ROOT
  [.S=H
    [.NP [.DT The ] [.NN plasma ] [.NN=H membrane ] ]
    [.VP=H
      [.VP=H
        [.VP=H [.VBZ=H moves ]
          [.ADVP [.RB=H away ]
            [.PP [.IN=H from ]
              [.NP [.DT the ] [.NN cell ] [.NN=H wall ] ] ] ] ]
        [.PP [.IN=H during ]
          [.NP
            [.NP=H [.NNS=H plasmolysis ] ]
            [.PP [.IN=H in ]
              [.NP
                [.NP=H [.DT a ] [.JJ walled ] [.NN=H cell ] ]
                [.PP [.IN=H inside ]
                  [.NP [.JJ hypertonic ] [.NN=H solution ] ] ] ] ] ] ] ]
      [., , ]
      [.S
        [.VP=H [.VBG=H resulting ]
          [.PP [.IN=H in ]
            [.NP [.DT a ] [.VBN damaged ] [.NN=H cell ] ] ] ] ] ]
    [.. . ] ] ]
\end{tikzpicture}

So far so good, nice and consistent. However, then we look at the NPs
and we see that the adjectives are sisters to the nouns that they
modify.

\begin{tikzpicture}[scale=0.5]
\Tree
[.ROOT
  [.S=H
    [.NP [.DT The ] [.NN=H [.NN plasma ] [.NN=H membrane ] ] ]
    [.VP=H
      [.VP=H
        [.VP=H [.VBZ=H moves ]
          [.ADVP [.RB=H away ]
            [.PP [.IN=H from ]
              [.NP [.DT the ] [.NN=H [.NN cell ] [.NN=H wall ] ] ] ] ] ]
        [.PP [.IN=H during ]
          [.NP
            [.NP=H [.NNS=H plasmolysis ] ]
            [.PP [.IN=H in ]
              [.NP
                [.NP=H [.DT a ] [.NN=H [.JJ walled ] [.NN=H cell ] ] ]
                [.PP [.IN=H inside ]
                  [.NP [.NN=H [.JJ hypertonic ] [.NN=H solution ] ] ] ] ] ] ] ] ]
      [., , ]
      [.S
        [.VP=H [.VBG=H resulting ]
          [.PP [.IN=H in ]
            [.NP [.DT a ] [.NN=H [.VBN damaged ] [.NN=H cell ] ] ] ] ] ] ]
    [.. . ] ] ]
\end{tikzpicture}

This asymmetry in the parser output made the approach of implementing
a procedure like theirs that would handle all of the flattening
phenomena in one unified manner seem not so viable. Instead, we chose
to implement a simple rule-based logic program to solve the flattening
we have discovered in our data.

We implemented two deflattening rules. One rule handled the general
case demonstrated on the PP and SBAR modifying the VP. This rule uses
a simple heuristic to determine whether a child of a node with more
than two children is a modifier or not. We refined this heuristic by
querying the output of the program and modifying the rules to account
for unforeseen circumstances. The final rule for detecting
modifiers/adjuncts follows.

A is an adjunct if one of the following is true:

\begin{itemize}
\item A is a PP
\item A is an S which does not have the form [S [VP=H [TO=H to] ...]]
\item A is a comma (we treat commas as adjuncts, so that, along with
  the ``real'' adjunct, they will form a two-level auxiliary tree)
\item A is an SBAR of the form [SBAR [WHNP=H [WDT=H which]] ...]
  (appositive relative clauses)
\end{itemize}

The second rule we have implemented is specialized for NPs. We look at
every noun in a final position (the last child of an NP or one that is
followed by a comma or a conjunction, since the Stanford parser
sometimes even flattens entire conjunctions of nouns, see below). For
every such noun, we look at its left sister and if it is a permissible
modifier (NN, NNP, JJ, VBN or ADJP). If so, we can mark the modifier
as an adjunct and put it together with the noun under a new noun tag.

This tree then...

\Tree
[.NP [.NN activation ] [.NN energy ]
     [.CC and ]
     [.NN water ] [.NNS=H molecules ] ]

...becomes this...

\Tree
[.NP [.NN [.NN activation ] [.NN energy ] ]
     [.CC and ]
     [.NNS=H [.NN water ] [.NNS=H molecules ] ] ]

After applying these two rules, we have examined all the possible
problem sites. That is, the nodes which had more than two children
after the deflattening (possible false negatives of our deflatter) and
all the adjunctions performed by our deflatter (possible false
positives). Upon their inspection, we have not found much room for
improvement and declared our algorithm finished.

The downside of this approach is that the further refining of our
heuristic predicates would get much harder as the size and variety of
our corpus grew larger. The upside is that it handles the language
found in our corpus very well and it has a short and (hopefully)
transparent implementation. For a discussion of some of the uncommon
implementation techniques we used, see the post at the following link.

\texttt{http://jirka.marsik.me/2012/12/19/the-working-hipster-s-clojure/}

\section{TAG extraction}
\section{Semantic roles alignment}
\section{Implementation and result}
data format
\section{Conclusion \& Future work}
\subsection{Future work}
\begin{itemize}
    \item workaround with parsing errors.
    \item more robust extraction algorithms
    \item more robust alignment algorithms
\end{itemize}
\end{document}
