\documentclass[a4paper]{article}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{lingmacros}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage[backend=biber,style=authoryear,natbib=true]{biblatex}
\bibliography{kbgen_proj}
\title{Lexicalized Tree Adjoining Grammar Extraction For KBGEN}
\author{Wei Qiu \and Jiri Marsik}
%\date{}
\begin{document}
\maketitle


\section{Introduction}
Natural language generation is the process to generate coherent natural language from knowledge base triples. There are generally two approaches in this area: some researchers use decent grammar formalism such as TAG(Tree adjoining grammar) to model the interaction of linguistic objects. This approach normally requires expert knowledge of grammar formalism, a lot of investigation into corpus and the domain where the system will be used. The main drawback of this approach is that it's not easy to migrate from domain to domain. The other approach is statistics based. It often directly work on the surface form of natural language. At the very beginning of this area, people use canned text to build simple NLG systems. The problem of this approach is that it's hard to model complicated linguistic phenomena and control the quality of the text generated.


Our project aims to reduce the effort of construct grammars for the first approach. We want to minimize the expert knowledge involved for building grammar for certain domain. A deep grammar formalism is still used, but the user doesn't need to handcraft the grammar from scratch anymore. 

This report is organized as below: firstly we will generally introduce the background of this task including the related KBGEN task and the corpus; Then the semantic alignment annotation will be described; Then two sections will describe our approach to extract Lexicalized Tree Adjoining Grammar which linguistically make sense.

\section{Background and previous work}
Our project is based on KBGen shared task.\citet{Banik2012} The goal of KBgen task to produce a coherent description of biological entities, processes and connections between them. The task involves aggregation adn the generation of intra-sentential, syntactically bound pronouns but it does not require the generation of any discourse anaphora of referring expressions. The corpus we used from KBgen task and contains 107 instances.

The idea using domain specific grammar extracted from parsed corpus for NLG system is proposed by \citet{DeVault2008a} and \citet{DeVault2008}. And extracting grammars from treebanks has already attracted a lot of attention from researchers. Several proposals for TAG extraction are made by \citet{Xia2001,Xia1999,Xia2006,Chen} not mentioning other deep grammar formalisms. Our approach is quite similar to \citet{DeVault2008a}'s method, but we payed more attention to semantic alignment annotation to make sure the grammar extracted has good linguistic meaning.


\section{Semantic Annotation}
\label{sec:sem-annot}

Extracting the elementary trees and aligning them to the triples
without any prior knowledge about the link between parts of the
sentences and the triples would have been very difficult. Therefore,
the first step of developing our system was to manually align sets of
triples to words from the sentences.

In order to shed light on the kind of alignment we were looking for, I
will list the properties we would like the alignment to have:

\begin{itemize}
\item Every word is aligned to some set of triples, i.e. every word is
  ``semantically motivated''. We would like our system to be able to
  generate the kind of sentences we had at the start. However, our
  surface realizer, GenI, never produces superfluous output (every
  word of the output is part of an elementary tree which was induced
  by some set of triples in the input).
\item Every triple is aligned to some set of words in the sentence.
  This is a reasonable requirement since the training sentences we
  have should be complete expressions of their semantic models.
  Furthermore, we would like to be able to generate at least the
  training data we have and GenI only considers generation successful
  if every semantic formula is realized via some elementary trees,
  which means there should not be any orphan triples.
\item The relation between the sets of words and sets of triples
  should be an alignment. By this, we mean to say that for every two
  different pairs of aligned sets of words and triples, the sets of
  words and the sets of triples should both be disjoint (in short, a
  word should not be aligned to two different sets of triples and a
  triple should not be aligned to two different sets of words).
\item The alignment we want to create should be the finest such
  alignment, meaning that any alignment which is a refinement of our
  alignment must lack one of the above properties. This is a natural
  requirement if we were doing things manually, since any rougher
  alignment can be easily arrived at by merging aligned groups.
  Furthermore, finer alignments will let us arrive at smaller
  elementary trees which generalize better, though they might
  overgenerate (which is a problem we were not afraid of, since we
  were afraid whether we will even be able to generate something).
\end{itemize}

For further discussion, we will label the sets of words that end up
aligned to some sets of triples \emph{groups}. Before we proceed with
a more linguistic description of the annotation guidelines, there is
one more technicality we would like to bring to light.

\subsection{Contiguous groups}

In our first rounds of annotation, we were working with an overly
strict constraint. Due to some confusion about how the realizer will
work and which realizer we will actually use, we restricted ourselves
to contiguous group only (groups whose all words form a contiguous
substring of the sentence). Under this constraint, we had trouble
accounting for several phenomena, the first one being conjunction.
Coordination was particularly tricky and required special treatment
even without the contiguity constraint (see Section \ref{ssec:coord}
for more details). However, other phrase structures such as ``... X
\textbf{, which also requires} Y \textbf{,} ...'' (mind the second
comma) with the semantics (X, requires, Y) or ``\textbf{the function
of} X \textbf{is} Y'' with semantics (X, has-function, Y).

After a while, we realized that the groups that we were manually
forming would correspond exactly to the final elementary trees (it was
the only alignment between semantics and syntax we had, after all).
Therefore, the sets of words in a group would not have to be
contiguous at all (the yield of an elementary tree may have
substitution or foot nodes surrounded by (co-)anchors).

\subsection{Annotation guidelines}

The realization that the groups of the alignment will correspond to
elementary trees restricts the space of possible alignments
significantly. These restrictions came both from the surface
realization of the sentence, where we imagined an idealized parse tree
and made sure that the groups would correspond to sensible and
composable elementary trees in that parse tree. On the semantic side,
we also needed the semantic links from the set of triples assigned by
the alignment to line up with the syntactic links in the elementary
tree (the substitution and adjunction sites).

Here are the guidelines for several common situations:

\begin{itemize}
\item NPs are aligned to their entity declarations.
\item For simple intransitive or transitive verbs, the event entity
  declaration and the triples which specify the agent (and the
  patient or whatever role the object takes) are aligned to the verb.
\item In an $n$-nary conjunction, all of the $n-1$ interposed
  conjunctions (comma, \emph{and}) are aligned to specialized
  \textbf{coordinates} relations which group all the coordinated
  entities into a single artificial entity.
\item For prepositions, we align the preposition (e.g. ``X \textbf{of}
  Y'') to a single triple which links some two entities (X and Y). In
  the resulting grammar, the preposition will correspond to an
  auxiliary tree where one the two semantic links will correspond to a
  substitution site and the other to the adjunction site.
\end{itemize}

And here is our guideline for a more involved but quite common
scenario:

\begin{itemize}
\item ``\textbf{The function of} X[NP] \textbf{is to provide energy
  for} Y[NP] \textbf{to} Z[VP].''

  Everything in bold in the sentence above will form a single group.
  This will be aligned to the following triples: (X, has-function, Z),
  (Z, raw-material, X) and (Z, agent, Y).
\end{itemize}

In the last example, you can see that I use the same meta-variables
for parts of the input syntax and the semantic entities. I assume
there is a natural assignment of the entities of the semantic
representation to certain constituents in the phrase structure. This
assumption is based on the fact that some groups (elementary trees)
are aligned to entity declarations and the idea that these entities
could be propagated up the tree according to headedness. It was this
idea that gave us hope that it should be possible to provide semantic
indices for the generated elementary trees automatically.


\section{Preprocessing}

There was a long way before we could start work on the problem itself
and we had to write a few auxiliary programs along the way. This
section describes the major steps and the tools we used there.

\subsection{Parsing and headedness}

Our plan is to extract the elementary trees for a TAG from a treebank.
Since our corpus lacks phrase structure information, we ran an
off-the-shelf parser on it. The parser that we used for this task is
the Stanford parser.

At the outset, we had plans to use both the phrase structure and
dependency parses that the Stanford parser could produce. Previous
research has shown that having a dependency parse facilitates the task
of determining headedness, which is usually a subtask of elementary
tree extraction.

Finally, we have found a simpler way of getting the head information
directly in the phrase structure parse trees through a command line
switch in the Stanford parser.

Furthermore, the Stanford parser offers two pre-trained models for
parsing English, an unlexicalized one and a lexicalized one. We have
done the parsing using both of them and try to review their
performance. Their outputs were significantly different and both
performed their own idiosyncratic errors. We judged which of these
errors would likely be more harmful to the following grammar
extraction process and decided to stick with the output of the
unlexicalized parser (though the result of the unlexicalized parsing
almost as just as bad as that of the lexicalized one).


\subsection{Aggregating coordinations}
\label{ssec:coord}

When we started working on the manual alignment with our self-enforced
constraint to produce only contiguous groups (see section
\ref{sec:sem-annot}), one of the difficulties we faced was
coordination. Consider the following example:

\begin{verbatim}
Electrogenic pumps, which consist of a hydrophobic amino acid,
a polar amino acid and a monomer, create  membrane potential.

(KBGEN-INPUT 
    :TRIPLES (
            (|Electrogenic-Pump21300| |has-part| |Monomer21323|)
            (|Electrogenic-Pump21300| |has-part| |Polar-Amino-Acid21291|)
            (|Electrogenic-Pump21300| |has-part| |Hydrophobic-Amino-Acid21290|)
            (|Create21297| |agent| |Electrogenic-Pump21300|)
            (|Create21297| |result| |Membrane-Potential21299|))
    :INSTANCE-TYPES (
            (|Monomer21323| |instance-of| |Monomer|)
            (|Polar-Amino-Acid21291| |instance-of| |Polar-Amino-Acid|)
            (|Hydrophobic-Amino-Acid21290| |instance-of| |Hydrophobic-Amino-Acid|)
            (|Electrogenic-Pump21300| |instance-of| |Electrogenic-Pump|)
            (|Create21297| |instance-of| |Create|)
            (|Membrane-Potential21299| |instance-of| |Membrane-Potential|)))
\end{verbatim}

To what group should the comma and the conjunction \emph{and} belong,
i.e. what semantics correspond to them and only to them? The
\textbf{has-part} relations belong to the verb \emph{consists}, the
instance declarations belong to the NPs. Yet we need to assign the
coordination constructions to some semantics, otherwise we will not be
able to generate them. To resolve this issue, we have developed a tool
to reify the coordination structures within the triples themselves.

In our first approach, our program would look for every instance where
one entity was in the same relation with multiple entities, such as
above, and converted the triples to the following:

\begin{verbatim}
:TRIPLES (
        (|Coordination2085| |coordinates| |Monomer21323|)
        (|Coordination2085| |coordinates| |Polar-Amino-Acid21291|)
        (|Coordination2086| |coordinates| |Coordination2085|)
        (|Coordination2086| |coordinates| |Hydrophobic-Amino-Acid21290|)
        (|Electrogenic-Pump21300| |has-part| |Coordination2086|)
        (|Create21297| |agent| |Electrogenic-Pump21300|)
        (|Create21297| |result| |Membrane-Potential21299|))
\end{verbatim}

What we did is that we built a very specific kind of binary tree which
covers the coordinated entities (the generated tree is specific in
that it always leans to the right, so it looks more like a linked
list). Here, one \textbf{Coordination} entity,
\textbf{Coordination2085}, subsumes the \textbf{Monomer21323} and the
\textbf{Polar-Amino-Acid21291}. This coordination corresponds to the
conjunction \emph{and}. The second coordination connects another
entity, the \textbf{Hydrophobic-Amino-Acid21290}, to the previously
formed \textbf{Coordination2085}. The resulting
\textbf{Coordination2086} can then serve as the singular object to the
verb \emph{consists}. Now, all of the NP coordinating constructs
(commas and \emph{and}) are easily accounted for. They form singleton
groups that are aligned to a pair of \textbf{coordinates} relations.

Once we made the switch from contiguous groups, we revised our
approach to coordination. Instead of making this hierarchy of
coordination constructs, we just create a single \textbf{Coordination}
entity which encapsulates all of the coordinated entities directly.
Obviously, this would not have been possible for coordinations of more
than 2 constituents using only contiguous groups, since we would not
to place two non-adjacent conjunction markers into one group.

The upsides of this new approach are that it reduces the complexity of
the newly added triples and that it does not overgenerate like the
last approach, which would end up extracting two elementary trees for
conjunction (one for comma and one for \emph{and}), but would not be
able to distinguish between them and use them properly. The only
downside of the new approach is that it needs to learn different
elementary trees for each different arity of conjunction present in
the corpus. However, since the number of coordinated constituents is
always between 2 and 4 in our corpus, this is a non-issue.

Here is the above example, now rendered using the new coordination
aggregation rules:

\begin{verbatim}
    :TRIPLES (
            (|Electrogenic-Pump21300| |has-part| |Coordination2015|)
            (|Coordination2015| |coordinates| |Monomer21323|)
            (|Coordination2015| |coordinates| |Polar-Amino-Acid21291|)
            (|Coordination2015| |coordinates| |Hydrophobic-Amino-Acid21290|)
            (|Create21297| |agent| |Electrogenic-Pump21300|)
            (|Create21297| |result| |Membrane-Potential21299|))
\end{verbatim}

The code for performing the former aggregation method is still present
in the aggregation program, but is no longer used (see function
\texttt{coordinate-objects-list}). Switching to the new method was
just a matter of writing a new function,
\texttt{coordinate-objects-flat}, and calling that one instead of the
former.


\subsection{Normalizing}

Once we had the manually aligned data with the aggregated
coordinations and parsed sentences, we thought ourselves ready to
extract the elementary trees. However, the trees produced by the
Stanford producer do not look very much like derived trees produced by
a TAG.

Xia and Palmer were facing a similar problem\citet{Xia2006}. We
considered applying their approach. However, their approach is not as
simple as it seems on paper. Consider the following parse tree we got
from

\begin{tikzpicture}[scale=0.5]
\Tree
[.ROOT
  [.S=H
    [.NP [.DT The ] [.NN plasma ] [.NN=H membrane ] ]
    [.VP=H [.VBZ=H moves ]
      [.ADVP [.RB=H away ]
        [.PP [.IN=H from ]
          [.NP [.DT the ] [.NN cell ] [.NN=H wall ] ] ] ]
      [.PP [.IN=H during ]
        [.NP
          [.NP=H [.NNS=H plasmolysis ] ]
          [.PP [.IN=H in ]
            [.NP
              [.NP=H [.DT a ] [.JJ walled ] [.NN=H cell ] ]
              [.PP [.IN=H inside ]
                [.NP [.JJ hypertonic ] [.NN=H solution ] ] ] ] ] ] ]
      [., , ]
      [.S
        [.VP=H [.VBG=H resulting ]
          [.PP [.IN=H in ]
            [.NP [.DT a ] [.VBN damaged ] [.NN=H cell ] ] ] ] ] ]
    [.. . ] ] ]
\end{tikzpicture}

The PP \emph{``during...''} and the S \emph{``resulting...''} with its
preceding comma both modify their parent, the VP of our sentence. We
can imagine that in our implementation of the algorithm, these would
be identified as modifiers and they would be put on different levels
from the argument ADVP, like so.

\begin{tikzpicture}[scale=0.5]
\Tree
[.ROOT
  [.S=H
    [.NP [.DT The ] [.NN plasma ] [.NN=H membrane ] ]
    [.VP=H
      [.VP=H
        [.VP=H [.VBZ=H moves ]
          [.ADVP [.RB=H away ]
            [.PP [.IN=H from ]
              [.NP [.DT the ] [.NN cell ] [.NN=H wall ] ] ] ] ]
        [.PP [.IN=H during ]
          [.NP
            [.NP=H [.NNS=H plasmolysis ] ]
            [.PP [.IN=H in ]
              [.NP
                [.NP=H [.DT a ] [.JJ walled ] [.NN=H cell ] ]
                [.PP [.IN=H inside ]
                  [.NP [.JJ hypertonic ] [.NN=H solution ] ] ] ] ] ] ] ]
      [., , ]
      [.S
        [.VP=H [.VBG=H resulting ]
          [.PP [.IN=H in ]
            [.NP [.DT a ] [.VBN damaged ] [.NN=H cell ] ] ] ] ] ]
    [.. . ] ] ]
\end{tikzpicture}

So far so good, nice and consistent. However, then we look at the NPs
and we see that the adjectives are sisters to the nouns that they
modify.

\begin{tikzpicture}[scale=0.5]
\Tree
[.ROOT
  [.S=H
    [.NP [.DT The ] [.NN=H [.NN plasma ] [.NN=H membrane ] ] ]
    [.VP=H
      [.VP=H
        [.VP=H [.VBZ=H moves ]
          [.ADVP [.RB=H away ]
            [.PP [.IN=H from ]
              [.NP [.DT the ] [.NN=H [.NN cell ] [.NN=H wall ] ] ] ] ] ]
        [.PP [.IN=H during ]
          [.NP
            [.NP=H [.NNS=H plasmolysis ] ]
            [.PP [.IN=H in ]
              [.NP
                [.NP=H [.DT a ] [.NN=H [.JJ walled ] [.NN=H cell ] ] ]
                [.PP [.IN=H inside ]
                  [.NP [.NN=H [.JJ hypertonic ] [.NN=H solution ] ] ] ] ] ] ] ] ]
      [., , ]
      [.S
        [.VP=H [.VBG=H resulting ]
          [.PP [.IN=H in ]
            [.NP [.DT a ] [.NN=H [.VBN damaged ] [.NN=H cell ] ] ] ] ] ] ]
    [.. . ] ] ]
\end{tikzpicture}

This asymmetry in the parser output made the approach of implementing
a procedure like theirs that would handle all of the flattening
phenomena in one unified manner seem not so viable. Instead, we chose
to implement a simple rule-based logic program to solve the flattening
we have discovered in our data.

We implemented two deflattening rules. One rule handled the general
case demonstrated on the PP and SBAR modifying the VP. This rule uses
a simple heuristic to determine whether a child of a node with more
than two children is a modifier or not. We refined this heuristic by
querying the output of the program and modifying the rules to account
for unforeseen circumstances. The final rule for detecting
modifiers/adjuncts follows.

A is an adjunct if one of the following is true:

\begin{itemize}
\item A is a PP
\item A is an S which does not have the form [S [VP=H [TO=H to] ...]]
\item A is a comma (we treat commas as adjuncts, so that, along with
  the ``real'' adjunct, they will form a two-level auxiliary tree)
\item A is an SBAR of the form [SBAR [WHNP=H [WDT=H which]] ...]
  (appositive relative clauses)
\end{itemize}

The second rule we have implemented is specialized for NPs. We look at
every noun in a final position (the last child of an NP or one that is
followed by a comma or a conjunction, since the Stanford parser
sometimes even flattens entire conjunctions of nouns, see below). For
every such noun, we look at its left sister and if it is a permissible
modifier (NN, NNP, JJ, VBN or ADJP). If so, we can mark the modifier
as an adjunct and put it together with the noun under a new noun tag.

This tree then...

\Tree
[.NP [.NN activation ] [.NN energy ]
     [.CC and ]
     [.NN water ] [.NNS=H molecules ] ]

...becomes this...

\Tree
[.NP [.NN [.NN activation ] [.NN energy ] ]
     [.CC and ]
     [.NNS=H [.NN water ] [.NNS=H molecules ] ] ]

After applying these two rules, we have examined all the possible
problem sites. That is, the nodes which had more than two children
after the deflattening (possible false negatives of our deflatter) and
all the adjunctions performed by our deflatter (possible false
positives). Upon their inspection, we have not found much room for
improvement and declared our algorithm finished.

The downside of this approach is that the further refining of our
heuristic predicates would get much harder as the size and variety of
our corpus grew larger. The upside is that it handles the language
found in our corpus very well and it has a short and (hopefully)
transparent implementation. For a discussion of some of the uncommon
implementation techniques we used, see the post at the following link.

\texttt{http://jirka.marsik.me/2012/12/19/the-working-hipster-s-clojure/}


\section{TAG extraction}
\label{sec:tag-ext}
After we get the normalized tree with head information, we can extract the TAG from it with the help of the alignment of surface sentence to underlying KBGEN triples.
The key idea is extract adjunction trees greedily, then extract the substitution trees. Of course conjunction trees as a specific form of substitution trees require
some special treatment before adjunction trees can be extracted.

First we will present the pseudo code Algorithm \ref{alg:tag_extract}, \ref{alg:extract_conj}, \ref{alg:extract_adj}, \ref{alg:extract_sub} for the tree extraction algorithms, then a working example will be given to illustrate the idea.
\begin{algorithm}
    \caption{LTAG extraction algorithm}
    \label{alg:tag_extract}
    \begin{algorithmic}[1]
        \REQUIRE $Tree$, $alignment$ 
        \FORALL{$node$ in $Tree$}
        \STATE add semantic group information according to $alignment$
        \ENDFOR
        \STATE $agenda \Leftarrow [Tree]$
        \STATE $well\_formed\_tree \Leftarrow []$
        \WHILE {$agenda$ is not empty}
            \STATE $current\_tree = agenda.pop() $
            \IF{$current\_tree$ only contains one group}
            \STATE $well\_formed\_tree.append(.current\_tree)$
            \ENDIF
            \STATE extract conjunction subpart
            \STATE extract adjunction subpart
            \IF {adjunction found}
                \STATE continue
            \ELSE
                \STATE extract substitution subpart
            \ENDIF
        \ENDWHILE
        \RETURN $well\_formed\_tree$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{extract conjunction subpart}
    \label{alg:extract_conj}
    \begin{algorithmic}[1]
        \FORALL {$subtree$ in $Tree$}
            \IF{$subtree$ has a conjunction structure}
                \STATE cut the conjunction tree off the subtree
                \STATE remove all arguments in the conjunction structure
                \STATE copy group information as candidate information
                %\COMMENT{This information is for semantic alignment afterwards}
                \STATE change the empty node'group to empty set
                \STATE update the group information of all separated parts 
                \STATE put subtree, conjunction tree, argument trees into $agenda$
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{extract adjunction subpart}
    \label{alg:extract_adj}
    \begin{algorithmic}[1]
        \FORALL {$subtree$ in $Tree$}
            \FORALL {leftmost child or rightmost child of $subtree$}
                \STATE TODO
                \IF {$child$ has the same pos as $subtree$}
                    \IF {$child$ is well separated from other parts}
                        \STATE cut off $subtree$ from its parent
                        \STATE copy the group information as candidate information
                        \STATE update the hole's group as empty set
                        \STATE cut off $child$ from $subtree$
                        \STATE copy the group information of $child$ hole as candidate information
                        \STATE update $child$ hole's group as empty set
                        \STATE update group information of all subparts
                        \STATE put adjunction mark
                        \STATE put all subparts into $agenda$
                    \ENDIF
                \ENDIF
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{extract substituition subpart}
    \label{alg:extract_sub}
    \begin{algorithmic}[1]
        \STATE $headgroup \Leftarrow current\_tree's head group$
        \FORALL {$subtree$ in $current\_tree$}
        \IF{$subtree$'s group is disjoint from $headgroup$}
            \STATE cut $subtree$ off $current_tree$
            \STATE copy the relative group information to candidate
            \STATE update all group information
            \STATE put substitution mark
        \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

We will take sentence

\enumsentence{The rate of detoxification in the liver cell is directly proportional to the quantity of smooth endoplasmic reticulum in the liver cell.}

as example, the semantic group annotation is:

\enumsentence{[1 The rate of] [2 detoxification] [3 in] [4 the liver cell] [5 is directly proportional to] [6 the quantity of] [7 smooth endoplasmic reticulum] [8 in] [9 the liver cell] .}
After parsing and preprocessing, we get the tree:

\begin{center}
\begin{tikzpicture}[scale=0.5]
\Tree [.ROOT [.S=H [.NP [.NP=H [.DT The ] [.NN=H rate ] ] [.PP [.IN=H of ] [.NP [.NP=H [.NN=H detoxification ] ] [.PP [.IN=H in ] [.NP [.DT the ] [.NN=H [.NN liver ] [.NN=H cell ] ] ] ] ] ] ] [.VP=H [.VBZ=H is ] [.ADJP [.ADJP [.RB directly ] [.JJ=H proportional ] ] [.PP [.TO=H to ] [.NP [.NP=H [.DT the ] [.NN=H quantity ] ] [.PP [.IN=H of ] [.NP [.NP=H [.JJ smooth ] [.NN=H [.JJ endoplasmic ] [.NN=H reticulum ] ] ] [.PP [.IN=H in ] [.NP [.DT the ] [.NN=H [.NN liver ] [.NN=H cell ] ] ] ] ] ] ] ] ] ] ] ]
\end{tikzpicture}
\end{center}

$S=H$ label means the syntax category is S and S is the head of its parent tree. We can see that stanford parser doesn't always provide head information. For example:
\begin{center}
\begin{tikzpicture}[scale=0.5]
\Tree [.ADJP [.ADJP [.RB directly ] [.JJ=H proportional ] ] [.PP [.TO=H to ] [.NP [.NP=H [.DT the ] [.NN=H quantity ] ] [.PP [.IN=H of ] [.NP [.NP=H [.JJ smooth ] [.NN=H [.JJ endoplasmic ] [.NN=H reticulum ] ] ] [.PP [.IN=H in ] [.NP [.DT the ] [.NN=H [.NN liver ] [.NN=H cell ] ] ] ] ] ] ] ] ]
\end{tikzpicture}
\end{center}
All of the children of root ADJP don't have any head information. This is a problem for our extraction algorithm simply because our algorithm relies on the head information to find the real group for the root. And quite often the head itself would be extracted out leaving a hole in the original tree. We need also to assign a head to the remaining siblings of it. Our work around is to assign the first child which has non-empty group informaiton as the head. This may seem to be problematic, but actually it works pretty well simply because the tree is almost binarized And stanford parser generate head information for the most cases. Of course, a heuristic rule may work better. We here leave it to future work. Check $extractor.\_get\_head\_group$ for the details.

First the whole tree will be added to agenda. After it has been poped out for further examination, firstly it will be checked whether it contains any conjunctions. The answer is no in this case. Then we can check whether it contains any adjuctions.

A simple strategy is to only check the root and its leftmost and rightmost children. But a close look into the corpus show us this strategy is too simple and sometimes will not work. It will extrat some grammars without good linguistic interpretation if we postpone the adjuction checking of other subtrees. So in our algorithm, we exaustively check the adjunction for all of the subtrees. An example that the simple strategy doesn't work is shown below:

\begin{center}
\begin{tikzpicture}[scale=0.5]
\Tree [.ROOT [.S=H [.NP [.NP=H [.DT The ] [.NN=H function ] ] [.PP [.IN=H of ] [.NP [.JJ membrane ] [.NN=H potential ] ] ] ] [.VP=H [.VBZ=H is ] [.S [.VP=H [.TO=H to ] [.VP [.VB=H provide ] [.NP [.NP=H [.NN=H energy ] ] [.PP [.IN=H for ] [.NP [.DT a ] [.NN=H [.NN concentration ] [.NN=H gradient ] ] ] ] ] [.S [.VP=H [.TO=H to ] [.VP [.VP [.VB=H diffuse ] [.NP [.NNS=H particles ] ] ] [.PP [.TO=H to ] [.NP [.DT the ] [.NN=H [.JJ extra ] [.NN=H [.JJ cellular ] [.NN=H matrix ] ] ] ] ] ] ] ] ] ] ] ] ] ]
\end{tikzpicture}
\end{center}
When 
\begin{center}
\begin{tikzpicture}[scale=0.5]
\Tree [.S [.VP=H [.TO=H to ] [.VP [.VP [.VB=H diffuse ] [.NP [.NNS=H particles ] ] ] [.PP [.TO=H to ] [.NP [.DT the ] [.NN=H [.JJ extra ] [.NN=H [.JJ cellular ] [.NN=H matrix ] ] ] ] ] ] ] ]
\end{tikzpicture}
\end{center}
is popped out, if we don't identify the adjuction structure wrapped inside, the substitution procedure would break it and it would be impossible to find a proper adjuction tree.

Now come back to our example, the chuck ``The rate of detoxification in the liver cell'' looks like a adjuction structure for its first left child, but actually it's not simply because the adjunction node's group is not disjoint from other parts(``of'' is also in group 1). It's illustrated in Figure \ref{fig:no-adjunct}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.4]{noadj.png}
    \end{center}
    \caption{``The rate'' is not adjuction}
    \label{fig:no-adjunct}
\end{figure}

Using the same algorithm, we can identify ``the rate of '' is a proper adjunction tree.

The third step is to extract the elementary tree. Actually after all of conjunctions and adjunctions are kicked out the leftover's root must be a root of elementary tree. Based on this observation, the algorithm for substitution tree extraction is relatively simple. First we identify the group of the root, then kick out all the parts which don't belong to this group and add them to the agenda. The left spine would be a ``pure'' elementary tree which can be directly added to the resulted grammar.

For this example, 9 grammar snippets will be extracted:

\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{1.eps}
    \end{center}
    \caption{Grammar fragment 1}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{2.eps}
    \end{center}
    \caption{Grammar fragment 2}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{3.eps}
    \end{center}
    \caption{Grammar fragment 3}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{4.eps}
    \end{center}
    \caption{Grammar fragment 4}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{5.png}
    \end{center}
    \caption{Grammar fragment 5}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{6.eps}
    \end{center}
    \caption{Grammar fragment 6}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{7.eps}
    \end{center}
    \caption{Grammar fragment 7}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{8.eps}
    \end{center}
    \caption{Grammar fragment 8}
\end{figure}
\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.5]{9.eps}
    \end{center}
    \caption{Grammar fragment 9}
\end{figure}
\section{Semantic roles alignment}
The extraction step extract grammar snippets for each sentence. This step intends to align the semantics to the grammar snippets and the syntax holes in the grammar snippets. The algorithm is showed in Algorithm \ref{alg:align}.

\begin{algorithm}
    \caption{Align semantic variables to syntax}
    \label{alg:align}
    \begin{algorithmic}[1]
        \FORALL{ $instance$ in alignment }
        \IF{it also occurs in triples}
            \STATE continue
        \ELSE
            \STATE align the tree which has the same group to it
        \ENDIF
        \ENDFOR
        \FORALL{$triple$ in alignment}
        \STATE align the tree which has the same group to it
            \FORALL{ hole in tree }
                \STATE assign the instance which is in the hole candidate set to it
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

The algorithm seems to be simple, but due to the Stanford parser's output is not reliable and the inconsistency in the semantic alignment annotation, this algorithm could not alway succeed in aligning correspond semantics to syntax. In previous step, grammar fragments can always be extracted out successfully.

The other problem we encountered in the alignment is that quite  often several triples are aligned to the same syntax. A example is that for sentences with structure ``the function of \ldots is for \ldots to \ldots''. This type of sentence would have 3 syntax holes in the elementary tree. And there are 3 triples related to it. We decided to extract 3 elementary trees for each there are two holes aligned to semantic variables. Since we haven't try out the generator, we don't know whether this treatment which we can consider it as a type of underspecification is appropriate or not here.

To filter out the same grammar fragments extracted from different sentences, we maintain a global hashtable.

For the example provided in section \ref{sec:tag-ext}, we got the final grammar below:

\begin{verbatim}
|directly-proportional|
(ROOT[]
  (S[+head]
    (NP[sem=0, +subs] )
    (VP[+head]
      (VBZ[+head] is)
      (ADJP[]
        (ADJP[] (RB[] directly) (JJ[+head] proportional))
        (PP[] (TO[+head] to) (NP[sem=1, +subs] ))))))

|Liver-Cell|
(DT[] the)
(NN[+head] (NN[] liver) (NN[+head] cell))

|quantity|
(NP[]
  (NP[+head] (DT[] the) (NN[+head] quantity))
  (PP[] (IN[+head] of) (NP[sem=0, +subs] )))

|has-part|
(NP[+adj]
  (NP[+adj, -head, sem=1] )
  (PP[+head] (IN[+head] in) (NP[sem=0, +subs] )))

|Smooth-Endoplasmic-Reticulum|
(NN[+head] (JJ[] endoplasmic) (NN[+head] reticulum))
(JJ[] smooth)

|Detoxification|
(NN[+head] detoxification)

|base|
(NP[+adj]
  (NP[+adj, -head, sem=0] )
  (PP[+head] (IN[+head] in) (NP[sem=1, +subs] )))

|rate|
(NP[]
  (NP[+head] (DT[] The) (NN[+head] rate))
  (PP[] (IN[+head] of) (NP[sem=0, +subs] )))
\end{verbatim}
Where sem represents the index of semantic variable.

\section{Implementation and result}
For implementation we use clojure and python. Clojure showed big advantages when we were trying to investigate the linguistic phenomena in the corpus. REPL really simplify the process. 
We use ParentedTree and FeatStruct from NLTK to model parse tree and feature structure. We found some small bugs in the deep copy facility provided by ParentedTree. Check our code for details.

The result is presented in directory ``./result/''. 'final.grm' contains the whole bunch of grammar fragments extracted out. 'grammr-verbose' contains the raw grammar which means semantics is not aligned for each sentence. Our algorithm works pretty well for correctly parsed sentences, but it would fail for those sentences which contain grammar errors. Here the errors don't need to be interpreted from linguistic perspective, but any inconsistency between the syntax tree and the semantic alignment can be considered as mistakes. To make our grammar not contain any mistakes, if one grammar fragment couldn't be aligned successfully, all other fragments from the same sentence will be deprecated.
However, even we do this, there are still quite a lot mistakes in the final grammar especially for small span for noun phrase. It's due to that for noun phrase there are no holes in the syntax to align then the mistakes are transparent in the aligning step.

To work around this problem, a filter can be built to filter out the illegal grammar fragments, especially if there are alternatives syntax for the same semantic underline. In parsing step, we can use other parsers which could produce K-best result(Stanford lexicalized/unlexicalized parser can't produce K-best result, but its PCFG parser can), then we can use the semantic alignment to guide us to pick out the consistent parsing tree.

Application based evaluation can be carried after we integrate our module into a surface realizer.

\section{Conclusion \& Future work}
In this project, we designed and implemented a algorithm to extract LTAG from parsed corpus and align the grammar to corresponding semantics. The biggest difference between our approach and \citet{DeVault2008a}'s approach is that our result has better linguistic meaning. At the same time, our approach requires a bit more expert knowledge in the semantic alignment annotation step. 

We still need to integrate our module with a surface realizer to build an end to end NLG system. Evaluation of our grammar needs to be carried.
For the extraction algorithm self, possible improvements include making the semantic alignment annotation automatic, make the algorithm more robust to work with parsing errors.
\printbibliography
\end{document}
